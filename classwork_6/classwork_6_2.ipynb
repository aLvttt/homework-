{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('dataset_eskov.txt'):\n",
    "    print(\"YES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_eskov.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Все началось с того, что Дмитрий, прогуливаясь по смотровой площадке, поскользнулся и сорвался с высоты. Он не кричал, не пытался цепляться — он будто смирился с неизбежным. Как рассказывают очевидцы, он падал пластом, словно огромный лист бумаги, распластавшись в воздухе. Сила удара о землю должна была быть разрушительной, но произошло нечто поразительное.\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    return ' '.join(w.lower() for w in (''.join(ch for ch in word if ch.isalpha()) for word in line.split()) if w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'все началось с того что дмитрий прогуливаясь по смотровой площадке поскользнулся и сорвался с высоты он не кричал не пытался цепляться он будто смирился с неизбежным как рассказывают очевидцы он падал пластом словно огромный лист бумаги распластавшись в воздухе сила удара о землю должна была быть разрушительной но произошло нечто поразительное'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lines,\n",
    "    ):\n",
    "        self.lines = lines\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.bos_token = '<BOS>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        self.uniq_words = [self.pad_token, self.bos_token, self.eos_token] + self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.pad_token_id = self.word_to_index['<PAD>']\n",
    "        self.bos_token_id = self.word_to_index['<BOS>']\n",
    "        self.eos_token_id = self.word_to_index['<EOS>']\n",
    "\n",
    "        self.tokenized = [[self.word_to_index[w] for w in line.split()] for line in self.lines]\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(word for line in self.lines for word in line.split())\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.LongTensor([self.bos_token_id] + self.tokenized[index]),\n",
    "            torch.LongTensor(self.tokenized[index] + [self.eos_token_id]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset([preprocess(line) for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<BOS>': 1,\n",
       " '<EOS>': 2,\n",
       " 'что': 3,\n",
       " 'на': 4,\n",
       " 'он': 5,\n",
       " 'дмитрий': 6,\n",
       " 'и': 7,\n",
       " 'с': 8,\n",
       " 'его': 9,\n",
       " 'не': 10,\n",
       " 'по': 11,\n",
       " 'стало': 12,\n",
       " 'который': 13,\n",
       " 'в': 14,\n",
       " 'все': 15,\n",
       " 'как': 16,\n",
       " 'очевидцы': 17,\n",
       " 'но': 18,\n",
       " 'асфальт': 19,\n",
       " 'бок': 20,\n",
       " 'след': 21,\n",
       " 'говорят': 22,\n",
       " 'однажды': 23,\n",
       " 'еськов': 24,\n",
       " 'известный': 25,\n",
       " 'своей': 26,\n",
       " 'неуемной': 27,\n",
       " 'тягой': 28,\n",
       " 'к': 29,\n",
       " 'высоте': 30,\n",
       " 'приключениям': 31,\n",
       " 'оказался': 32,\n",
       " 'вершине': 33,\n",
       " 'знаменитого': 34,\n",
       " 'эмпайрстейтбилдинга': 35,\n",
       " 'легенде': 36,\n",
       " 'невероятное': 37,\n",
       " 'падение': 38,\n",
       " 'городским': 39,\n",
       " 'мифом': 40,\n",
       " 'передают': 41,\n",
       " 'из': 42,\n",
       " 'уст': 43,\n",
       " 'уста': 44,\n",
       " 'началось': 45,\n",
       " 'того': 46,\n",
       " 'прогуливаясь': 47,\n",
       " 'смотровой': 48,\n",
       " 'площадке': 49,\n",
       " 'поскользнулся': 50,\n",
       " 'сорвался': 51,\n",
       " 'высоты': 52,\n",
       " 'кричал': 53,\n",
       " 'пытался': 54,\n",
       " 'цепляться': 55,\n",
       " 'будто': 56,\n",
       " 'смирился': 57,\n",
       " 'неизбежным': 58,\n",
       " 'рассказывают': 59,\n",
       " 'падал': 60,\n",
       " 'пластом': 61,\n",
       " 'словно': 62,\n",
       " 'огромный': 63,\n",
       " 'лист': 64,\n",
       " 'бумаги': 65,\n",
       " 'распластавшись': 66,\n",
       " 'воздухе': 67,\n",
       " 'сила': 68,\n",
       " 'удара': 69,\n",
       " 'о': 70,\n",
       " 'землю': 71,\n",
       " 'должна': 72,\n",
       " 'была': 73,\n",
       " 'быть': 74,\n",
       " 'разрушительной': 75,\n",
       " 'произошло': 76,\n",
       " 'нечто': 77,\n",
       " 'поразительное': 78,\n",
       " 'легенда': 79,\n",
       " 'гласит': 80,\n",
       " 'упал': 81,\n",
       " 'прямо': 82,\n",
       " 'перед': 83,\n",
       " 'зданием': 84,\n",
       " 'удар': 85,\n",
       " 'был': 86,\n",
       " 'настолько': 87,\n",
       " 'мощным': 88,\n",
       " 'треснул': 89,\n",
       " 'а': 90,\n",
       " 'буквально': 91,\n",
       " 'стесался': 92,\n",
       " 'об': 93,\n",
       " 'него': 94,\n",
       " 'оставляя': 95,\n",
       " 'характерный': 96,\n",
       " 'дороге': 97,\n",
       " 'однако': 98,\n",
       " 'несмотря': 99,\n",
       " 'законы': 100,\n",
       " 'физики': 101,\n",
       " 'чудом': 102,\n",
       " 'остался': 103,\n",
       " 'жив': 104,\n",
       " 'вспоминают': 105,\n",
       " 'поднялся': 106,\n",
       " 'отряхнулся': 107,\n",
       " 'взглянул': 108,\n",
       " 'свой': 109,\n",
       " 'изрядно': 110,\n",
       " 'пострадавший': 111,\n",
       " 'лишь': 112,\n",
       " 'задумчиво': 113,\n",
       " 'произнес': 114,\n",
       " 'ну': 115,\n",
       " 'вроде': 116,\n",
       " 'терпимо': 117,\n",
       " 'тех': 118,\n",
       " 'пор': 119,\n",
       " 'место': 120,\n",
       " 'где': 121,\n",
       " 'приземлился': 122,\n",
       " 'достопримечательностью': 123,\n",
       " 'горожане': 124,\n",
       " 'прозвали': 125,\n",
       " 'еськовский': 126,\n",
       " 'поговаривают': 127,\n",
       " 'даже': 128,\n",
       " 'сегодня': 129,\n",
       " 'там': 130,\n",
       " 'можно': 131,\n",
       " 'увидеть': 132,\n",
       " 'отпечаток': 133,\n",
       " 'напоминающий': 134,\n",
       " 'силуэт': 135,\n",
       " 'человека': 136,\n",
       " 'смог': 137,\n",
       " 'только': 138,\n",
       " 'выжить': 139,\n",
       " 'стать': 140,\n",
       " 'частью': 141,\n",
       " 'городской': 142,\n",
       " 'легенды': 143}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<PAD>', 0),\n",
       " ('<BOS>', 1),\n",
       " ('<EOS>', 2),\n",
       " ('что', 3),\n",
       " ('на', 4),\n",
       " ('он', 5),\n",
       " ('дмитрий', 6),\n",
       " ('и', 7),\n",
       " ('с', 8),\n",
       " ('его', 9)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index_list = list(dataset.word_to_index.items())\n",
    "word_to_index_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1, 22,  3, 23,  6, 24, 25, 26, 27, 28, 29, 30,  7, 31, 32,  4, 33, 34,\n",
       "         35, 11, 36,  9, 37, 38, 12, 39, 40, 13, 41, 42, 43, 14, 44]),\n",
       " tensor([22,  3, 23,  6, 24, 25, 26, 27, 28, 29, 30,  7, 31, 32,  4, 33, 34, 35,\n",
       "         11, 36,  9, 37, 38, 12, 39, 40, 13, 41, 42, 43, 14, 44,  2]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=dataset.pad_token_id)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=dataset.pad_token_id)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=512, collate_fn=pad_collate, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_len):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = 256\n",
    "        self.embedding_dim = 256\n",
    "        self.num_layers = 3\n",
    "\n",
    "        vocab_len = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_len,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, vocab_len)\n",
    "\n",
    "    def forward(self, x, lens=None, prev_state=None):\n",
    "        embed = self.embedding(x)\n",
    "        if lens is None:\n",
    "            output, state = self.rnn(embed, prev_state)\n",
    "        else:\n",
    "            embed_packed = pack_padded_sequence(embed, lens, batch_first=True, enforce_sorted=False)\n",
    "            output_packed, state = self.rnn(embed_packed, prev_state)\n",
    "            output, _ = pad_packed_sequence(output_packed, batch_first=True)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = Model(len(dataset.uniq_words)).to(DEVICE).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652880"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epochs):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y, x_lens, y_lens in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, _ = model(x.to(DEVICE), x_lens)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y.to(DEVICE))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if epoch % 10 == 0:\n",
    "            print({ 'epoch': epoch, 'loss': losses[-1] })\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'loss': 4.968752861022949}\n",
      "{'epoch': 10, 'loss': 2.1885015964508057}\n",
      "{'epoch': 20, 'loss': 1.1332231760025024}\n",
      "{'epoch': 30, 'loss': 0.6654178500175476}\n",
      "{'epoch': 40, 'loss': 0.4026296138763428}\n",
      "{'epoch': 50, 'loss': 0.23544885218143463}\n",
      "{'epoch': 60, 'loss': 0.16321681439876556}\n",
      "{'epoch': 70, 'loss': 0.11219439655542374}\n",
      "{'epoch': 80, 'loss': 0.0785694420337677}\n",
      "{'epoch': 90, 'loss': 0.058620989322662354}\n",
      "{'epoch': 100, 'loss': 0.05196380987763405}\n",
      "{'epoch': 110, 'loss': 0.05078313127160072}\n",
      "{'epoch': 120, 'loss': 0.04560316726565361}\n",
      "{'epoch': 130, 'loss': 0.04261617362499237}\n",
      "{'epoch': 140, 'loss': 0.039727695286273956}\n"
     ]
    }
   ],
   "source": [
    "loss_history = train(model, dataloader, criterion, optimizer, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(value):\n",
    "    return [dataset.bos_token_id]+[dataset.word_to_index[word.lower()] for word in value.split()]\n",
    "\n",
    "\n",
    "def decode(token_ids):\n",
    "    return ' '.join(dataset.index_to_word[token_id] for token_id in token_ids)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(prompt, max_tokens=20):\n",
    "    model.eval()\n",
    "    response = []\n",
    "    state = None\n",
    "    prompt_tokens = tokenize(prompt)\n",
    "    model_input = torch.LongTensor([prompt_tokens]).to(DEVICE)\n",
    "    for _ in range(max_tokens):\n",
    "        logits, state = model(model_input, prev_state=state)\n",
    "        token_argmax = logits[0, -1].argmax()\n",
    "        response.append(token_argmax.item())\n",
    "        if response[-1] == dataset.eos_token_id:\n",
    "            break\n",
    "        model_input = token_argmax.view(1, 1)\n",
    "\n",
    "    return decode(prompt_tokens + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> еськов что однажды дмитрий еськов известный своей неуемной тягой к высоте и приключениям оказался на вершине знаменитого эмпайрстейтбилдинга по легенде его'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('еськов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(prompt, max_tokens=20):\n",
    "    model.eval()\n",
    "    response = []\n",
    "    state = None\n",
    "    prompt_tokens = tokenize(prompt)\n",
    "    model_input = torch.LongTensor([prompt_tokens]).to(DEVICE)\n",
    "    for _ in range(max_tokens):\n",
    "        logits, state = model(model_input, prev_state=state)\n",
    "        token_probs = F.softmax(logits[0, -1], dim=-1).cpu().numpy()\n",
    "        sampled_token = np.random.choice(len(token_probs), p=token_probs)\n",
    "        response.append(sampled_token)\n",
    "        if response[-1] == dataset.eos_token_id:\n",
    "            break\n",
    "        model_input = torch.LongTensor([[sampled_token]]).to(DEVICE)\n",
    "\n",
    "    return decode(prompt_tokens + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> еськов что однажды дмитрий еськов своей своей неуемной тягой к высоте и приключениям оказался на вершине знаменитого эмпайрстейтбилдинга по легенде его'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample('еськов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\anastasia\\documents\\github\\homework_muiv_iroli\\venv\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, AutoModelForSeq2SeqLM\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msberbank-ai/ruT5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\GitHub\\homework_MUIV_iroli\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\Documents\\GitHub\\homework_MUIV_iroli\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"sberbank-ai/ruT5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.uniq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(\n",
    "    x for tokens in (\n",
    "        tokenizer(\n",
    "            preprocess(line),\n",
    "            add_special_tokens=False,\n",
    "        )['input_ids'] for line in lines\n",
    "    ) for x in tokens\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
